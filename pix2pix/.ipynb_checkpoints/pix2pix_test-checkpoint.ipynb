{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is used to test out Pix2Pix GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "7\n",
      "16\n",
      "34\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "# example of calculating the receptive field for the PatchGAN\n",
    "\n",
    "# calculate the effective receptive field size\n",
    "def receptive_field(output_size, kernel_size, stride_size):\n",
    "    return (output_size - 1) * stride_size + kernel_size\n",
    "\n",
    "# output layer 1x1 pixel with 4x4 kernel and 1x1 stride\n",
    "rf = receptive_field(1, 4, 1)\n",
    "print(rf)\n",
    "# second last layer with 4x4 kernel and 1x1 stride\n",
    "rf = receptive_field(rf, 4, 1)\n",
    "print(rf)\n",
    "# 3 PatchGAN layers with 4x4 kernel and 2x2 stride\n",
    "rf = receptive_field(rf, 4, 2)\n",
    "print(rf)\n",
    "rf = receptive_field(rf, 4, 2)\n",
    "print(rf)\n",
    "rf = receptive_field(rf, 4, 2)\n",
    "print(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-06 15:24:50.131303: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2023-08-06 15:24:50.131326: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2023-08-06 15:24:50.131336: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2023-08-06 15:24:50.131370: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-08-06 15:24:50.131388: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/Users/javkhlan-ochirganbat/.venv/tensorflow/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "/Users/javkhlan-ochirganbat/.venv/tensorflow/lib/python3.10/site-packages/keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# example of defining a 70x70 patchgan discriminator model\n",
    "# from keras.optimizers import Adam\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model\n",
    "from keras import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# define the discriminator model\n",
    "def define_discriminator(image_shape):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# source image input\n",
    "\tin_src_image = Input(shape=image_shape)\n",
    "\t# target image input\n",
    "\tin_target_image = Input(shape=image_shape)\n",
    "\t# concatenate images channel-wise\n",
    "\tmerged = Concatenate()([in_src_image, in_target_image])\n",
    "\t# C64\n",
    "\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C128\n",
    "\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C256\n",
    "\td = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C512\n",
    "\td = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# second last output layer\n",
    "\td = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# patch output\n",
    "\td = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "\tpatch_out = Activation('sigmoid')(d)\n",
    "\t# define model\n",
    "\tmodel = Model([in_src_image, in_target_image], patch_out)\n",
    "\t# compile model\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "\treturn model\n",
    "\n",
    "# define image shape\n",
    "image_shape = (256,256,3)\n",
    "# create the model\n",
    "model = define_discriminator(image_shape)\n",
    "# summarize the model\n",
    "# model.summary()\n",
    "# plot the model\n",
    "# plot_model(model, to_file='discriminator_model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of defining a u-net encoder-decoder generator model\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model\n",
    "from keras import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# define an encoder block\n",
    "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# add downsampling layer\n",
    "\tg = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "\t# conditionally add batch normalization\n",
    "\tif batchnorm:\n",
    "\t\tg = BatchNormalization()(g, training=True)\n",
    "\t# leaky relu activation\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\treturn g\n",
    "\n",
    "# define a decoder block\n",
    "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# add upsampling layer\n",
    "\tg = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "\t# add batch normalization\n",
    "\tg = BatchNormalization()(g, training=True)\n",
    "\t# conditionally add dropout\n",
    "\tif dropout:\n",
    "\t\tg = Dropout(0.5)(g, training=True)\n",
    "\t# merge with skip connection\n",
    "\tg = Concatenate()([g, skip_in])\n",
    "\t# relu activation\n",
    "\tg = Activation('relu')(g)\n",
    "\treturn g\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(image_shape=(256,256,3)):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# image input\n",
    "\tin_image = Input(shape=image_shape)\n",
    "\t# encoder model: C64-C128-C256-C512-C512-C512-C512-C512\n",
    "\te1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
    "\te2 = define_encoder_block(e1, 128)\n",
    "\te3 = define_encoder_block(e2, 256)\n",
    "\te4 = define_encoder_block(e3, 512)\n",
    "\te5 = define_encoder_block(e4, 512)\n",
    "\te6 = define_encoder_block(e5, 512)\n",
    "\te7 = define_encoder_block(e6, 512)\n",
    "\t# bottleneck, no batch norm and relu\n",
    "\tb = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
    "\tb = Activation('relu')(b)\n",
    "\t# decoder model: CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n",
    "\td1 = decoder_block(b, e7, 512)\n",
    "\td2 = decoder_block(d1, e6, 512)\n",
    "\td3 = decoder_block(d2, e5, 512)\n",
    "\td4 = decoder_block(d3, e4, 512, dropout=False)\n",
    "\td5 = decoder_block(d4, e3, 256, dropout=False)\n",
    "\td6 = decoder_block(d5, e2, 128, dropout=False)\n",
    "\td7 = decoder_block(d6, e1, 64, dropout=False)\n",
    "\t# output\n",
    "\tg = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n",
    "\tout_image = Activation('tanh')(g)\n",
    "\t# define model\n",
    "\tmodel = Model(in_image, out_image)\n",
    "\treturn model\n",
    "\n",
    "# define image shape\n",
    "image_shape = (256,256,3)\n",
    "# create the model\n",
    "model = define_generator(image_shape)\n",
    "# summarize the model\n",
    "# model.summary()\n",
    "# plot the model\n",
    "# plot_model(model, to_file='generator_model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model, image_shape):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\tfor layer in d_model.layers:\n",
    "\t\tif not isinstance(layer, BatchNormalization):\n",
    "\t\t\tlayer.trainable = False\n",
    "\t# define the source image\n",
    "\tin_src = Input(shape=image_shape)\n",
    "\t# connect the source image to the generator input\n",
    "\tgen_out = g_model(in_src)\n",
    "\t# connect the source input and generator output to the discriminator input\n",
    "\tdis_out = d_model([in_src, gen_out])\n",
    "\t# src image as input, generated image and classification output\n",
    "\tmodel = Model(in_src, [dis_out, gen_out])\n",
    "\t# compile model\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " model_3 (Functional)        (None, 256, 256, 3)          5442931   ['input_7[0][0]']             \n",
      "                                                          5                                       \n",
      "                                                                                                  \n",
      " model_2 (Functional)        (None, 16, 16, 1)            6968257   ['input_7[0][0]',             \n",
      "                                                                     'model_3[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 61397572 (234.21 MB)\n",
      "Trainable params: 54422275 (207.60 MB)\n",
      "Non-trainable params: 6975297 (26.61 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define image shape\n",
    "image_shape = (256,256,3)\n",
    "# define the models\n",
    "d_model = define_discriminator(image_shape)\n",
    "g_model = define_generator(image_shape)\n",
    "# define the composite model\n",
    "gan_model = define_gan(g_model, d_model, image_shape)\n",
    "# summarize the model\n",
    "gan_model.summary()\n",
    "# plot the model\n",
    "# plot_model(gan_model, to_file='gan_model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# select a batch of random samples, returns images and target\n",
    "def generate_real_samples(dataset, n_samples, patch_shape):\n",
    "\t# unpack dataset\n",
    "\ttrainA, trainB = dataset\n",
    "\t# choose random instances\n",
    "\tix = randint(0, trainA.shape[0], n_samples)\n",
    "\t# retrieve selected images\n",
    "\tX1, X2 = trainA[ix], trainB[ix]\n",
    "\t# generate 'real' class labels (1)\n",
    "\ty = np.ones((n_samples, patch_shape, patch_shape, 1))\n",
    "\treturn [X1, X2], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a batch of images, returns images and targets\n",
    "def generate_fake_samples(g_model, samples, patch_shape):\n",
    "\t# generate fake instance\n",
    "\tX = g_model.predict(samples)\n",
    "\t# create 'fake' class labels (0)\n",
    "\ty = np.zeros((len(X), patch_shape, patch_shape, 1))\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, dataset, n_samples=3):\n",
    "\t# select a sample of input images\n",
    "\t[X_realA, X_realB], _ = generate_real_samples(dataset, n_samples, 1)\n",
    "\t# generate a batch of fake samples\n",
    "\tX_fakeB, _ = generate_fake_samples(g_model, X_realA, 1)\n",
    "\t# scale all pixels from [-1,1] to [0,1]\n",
    "\tX_realA = (X_realA + 1) / 2.0\n",
    "\tX_realB = (X_realB + 1) / 2.0\n",
    "\tX_fakeB = (X_fakeB + 1) / 2.0\n",
    "\t# plot real source images\n",
    "\tfor i in range(n_samples):\n",
    "\t\tax = pyplot.subplot(3, n_samples, 1 + i)\n",
    "\t\tax.set_title(\"Real Image\")\n",
    "\t\tpyplot.axis('off')\n",
    "\t\tpyplot.imshow(X_realA[i])\n",
    "\t# plot generated target image\n",
    "\tfor i in range(n_samples):\n",
    "\t\tax = pyplot.subplot(3, n_samples, 1 + n_samples + i)\n",
    "\t\tax.set_title(\"Generated Image\")\n",
    "\t\tpyplot.axis('off')\n",
    "\t\tpyplot.imshow(X_fakeB[i])\n",
    "\t# plot real target image\n",
    "\tfor i in range(n_samples):\n",
    "\t\tax = pyplot.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
    "\t\tax.set_title(\"Target Image\")\n",
    "\t\tpyplot.axis('off')\n",
    "\t\tpyplot.imshow(X_realB[i])\n",
    "\t# save plot to file\n",
    "\tfilename1 = 'plot_%06d.png' % (step+1)\n",
    "\tpyplot.savefig(filename1)\n",
    "\tpyplot.close()\n",
    "\t# save the generator model\n",
    "\tfilename2 = 'model_%06d.h5' % (step+1)\n",
    "\tg_model.save(filename2)\n",
    "\tprint('>Saved: %s and %s' % (filename1, filename2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train pix2pix models\n",
    "def train(d_model, g_model, gan_model, dataset, n_epochs=10, n_batch=1, n_patch=16):\n",
    "\t# unpack dataset\n",
    "\ttrainA, trainB = dataset\n",
    "\t# calculate the number of batches per training epoch\n",
    "\tbatch_per_epo = int(len(trainA) / n_batch)\n",
    "\t# calculate the number of training iterations\n",
    "\tn_steps = batch_per_epo * n_epochs\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(1, n_steps+1):\n",
    "\t\t# select a batch of real samples\n",
    "\t\t[X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
    "\t\t# generate a batch of fake samples\n",
    "\t\tX_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
    "\t\t# update discriminator for real samples\n",
    "\t\td_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
    "\t\t# update discriminator for generated samples\n",
    "\t\td_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
    "\t\t# update the generator\n",
    "\t\tg_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
    "\t\t# summarize performance\n",
    "\t\tif i % 10 == 0:\n",
    "\t\t\tprint(f\"Step={i} of {n_steps} Disc Loss(real sample)={d_loss1} {d_loss2} Disc Loss(gen sample)={d_loss2} Gen Loss={g_loss}\")\n",
    "\t\t# summarize model performance\n",
    "\t\tif i % (batch_per_epo * 10) == 0:\n",
    "\t\t\tsummarize_performance(i, g_model, dataset)\n",
    "\n",
    "\t\t# print('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load, split and scale the maps dataset ready for training\n",
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import vstack\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "from numpy import savez_compressed\n",
    "\n",
    "# load all images in a directory into memory\n",
    "def load_images(path, size=(256,512)):\n",
    "\tsrc_list, tar_list = list(), list()\n",
    "\t# enumerate filenames in directory, assume all are images\n",
    "\tfor filename in listdir(path):\n",
    "\t\t# load and resize the image\n",
    "\t\tpixels = load_img(path + filename, target_size=size)\n",
    "\t\t# convert to numpy array\n",
    "\t\tpixels = img_to_array(pixels)\n",
    "\t\t# split into satellite and map\n",
    "\t\tsat_img, map_img = pixels[:, :256], pixels[:, 256:]\n",
    "\t\tsrc_list.append(sat_img)\n",
    "\t\ttar_list.append(map_img)\n",
    "\treturn [asarray(src_list), asarray(tar_list)]\n",
    "\n",
    "# # dataset path\n",
    "# path = './data/maps/train/'\n",
    "# # load dataset\n",
    "# [src_images, tar_images] = load_images(path)\n",
    "# print('Loaded: ', src_images.shape, tar_images.shape)\n",
    "# # save as compressed numpy array\n",
    "# filename = 'maps_256.npz'\n",
    "# savez_compressed(filename, src_images, tar_images)\n",
    "# print('Saved dataset: ', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the prepared dataset\n",
    "# from numpy import load\n",
    "# from matplotlib import pyplot\n",
    "# # load the dataset\n",
    "# data = load('maps_256.npz')\n",
    "# src_images, tar_images = data['arr_0'], data['arr_1']\n",
    "# print('Loaded: ', src_images.shape, tar_images.shape)\n",
    "# # plot source images\n",
    "# n_samples = 3\n",
    "# for i in range(n_samples):\n",
    "# \tpyplot.subplot(2, n_samples, 1 + i)\n",
    "# \tpyplot.axis('off')\n",
    "# \tpyplot.imshow(src_images[i].astype('uint8'))\n",
    "# # plot target image\n",
    "# for i in range(n_samples):\n",
    "# \tpyplot.subplot(2, n_samples, 1 + n_samples + i)\n",
    "# \tpyplot.axis('off')\n",
    "# \tpyplot.imshow(tar_images[i].astype('uint8'))\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# load image data\n",
    "# dataset = ...\n",
    "# load image data\n",
    "# load and prepare training images\n",
    "def load_real_samples(filename):\n",
    " # load compressed arrays\n",
    " data = load(filename)\n",
    " # unpack arrays\n",
    " X1, X2 = data['arr_0'], data['arr_1']\n",
    " # scale from [0,255] to [-1,1]\n",
    " X1 = (X1 - 127.5) / 127.5\n",
    " X2 = (X2 - 127.5) / 127.5\n",
    " return [X1, X2]\n",
    "\n",
    "\n",
    "dataset = load_real_samples('maps_256.npz')\n",
    "print('Loaded', dataset[0].shape, dataset[1].shape)\n",
    "# define input shape based on the loaded dataset\n",
    "image_shape = dataset[0].shape[1:]\n",
    "# define the models\n",
    "d_model = define_discriminator(image_shape)\n",
    "g_model = define_generator(image_shape)\n",
    "# define the composite model\n",
    "gan_model = define_gan(g_model, d_model, image_shape)\n",
    "# train model\n",
    "train(d_model, g_model, gan_model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      ">Saved: plot_000003.png and model_000003.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/javkhlan-ochirganbat/.venv/tensorflow/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "summarize_performance(i, g_model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = \"\"\n",
    "X_fakeB, _ = generate_fake_samples(g_model, test_image, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
